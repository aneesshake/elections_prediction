---
title: "Prediction of the 2020 Presidential Election"
subtitle: "Spoiler Alert: Trump Wins"
author: "Anees Shaikh, Jaffa Romain, Lu Mu, and Cameron Fryer"
thanks: "Code and data are available at: https://github.com/aneesshake/elections_prediction"
date: "`r format(Sys.time(), '%d %B %Y')`"
abstract: |
  | The purpose of this analysis is to forecast the results of the upcoming United States 2020 presidential election. In this paper, we first consider the Democracy Fund + UCLA Nationscape Wave 50 dataset, which contains the results of a survey (conducted June 25-July 1, 2020) on American voter attitudes. Thereafter, the survey data is used to train our model relating voter intent to 4 explanatory variables. The model is then applied to the post-stratification dataset; namely, the results of the 2018 1-year American Community Survey (ACS), to get predictions representative of the target population. Since the ACS data pertains to individual persons and their characteristics, the associated use of our model allows us to conclude that Donald Trump will be relected as the President of the United States, winning the popular vote by ___ with a margin of error of ______
Keywords: forecasting; US 2020 election; Trump; Biden; multilevel regression with post-stratification
output:
  bookdown::pdf_document2:
    citation_package: natbib
toc: FALSE
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tinytex)
library(readr)
library(here)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(ResourceSelection)
library(maps)
library(mapproj)
library(caret)
library(statebins)
```

# Introduction

There are a vast number of ways in which statistical analysis can be sued to foretell future political future  political appointment. With that being said, however, the statistical methods one chooses to exploit are usually dependent on the data in question. As such, it is important to both understand, and be able to describe the data before considering the optimal approach in working with it. By staying true to this procedure deriving inferences about some given 
# Data

 ## Individual Survey Data
 
As previously mentioned, our data is from two different datasets; the Democracy Fund + UCLA Nationscape Wave 50 dataset, and the 2018 1-year American Community Survey (ACS) dataset. They each contain both qualitative and quantitative data. 
To forecast the results of the presidential election, our model was fitted using the Nationscape survey data. The Democracy Fund + UCLA Nationscape is "one of the largest public opinion surveys ever conducted -- interviewing people in nearly every county, congressional district, and mid-sized U.S. city in the lead up to the 2020 election (Tausanovitch, 2020)." Consequently, interviewing "roughly 6,250 per week" was part of the survey methodology employed by Nationscape. As such, the Nationscape Wave 50 dataset contains results for 6,479 interviews conducted during the week of June 25-July 1, 2020. It can be accessed by visiting https://www.voterstudygroup.org/publication/nationscape-data-set and submitting a request to access the data (Tausanovitch, 2020). By providing your full name and email address, you will then be emailed a link to the page where it can be downloaded. 

Nationscape samples are provided by Lucid, a market research platform that uses their software for conducting online surveys. Nationscape surveys were completed by participants online in English (Tausanovitch, 2020). The survey data is then weighted using a simple raking technique, where one set of weights is generated for each week's survey, in an attempt to have data that is representative of their target population. The survey data is weighted on many demographic factors such as gender, age, and nativity.

  At first glance of the Wave 50 data, we noted that it contains 265 variables. This is noteworthy because it allows a more complete picture of voter attitude to be derived from the data compared to if there were less variables. Another strength of this dataset is the overall quality of responses from those surveyed. More specifically, the data does not contain any duplicate records, inappropriate entries, or poor entries (e.g. typos, misspellings); hence, making it easier to work with. This high quality of survey responses can be at least partially attributed to the fact that "Nationscape samples are provided by Lucid, a market research platform that runs an online exchange for survey respondents(Tausanovitch, 2020)". What is more, the individuals belonging to the sample are required to "complete an attention check before completing the [online] survey (Tausanovitch, 2020)." With this method of sample selection, along with the precaution taken to ensure their attentiveness, it is unlikely for survey respondents to submit low-quality responses. 
  
  The main weakness of the Nationscape Wave 50 dataset is the likelihood that it includes sampling error and/or coverage error. While the goal of the Nationscape survey is to determine American voter attitudes, making inferences based only on the Wave 50 dataset would result in sample bias. This is because the target population is all American citizens age 18 years and older, whereas the sample frame is Lucid's online exchange for survey respondents, and the sample is a set of 6,479 individuals from said online exchange which match "a set of demographic criteria (Tausanovitch, 2020)." It is unlikely that a survey sample of 6,479 American citizens during a single week is representative of American voter attitudes as a whole. This may lead one to wonder why Nationscape chose to use the described sampling approach. Apart from the fact that the survey was conducted every week, the reason is trade-offs. For example, although there is potential coverage error in the data--a common result of online surveys--some trade-offs are a lesser rate of non-response, lower cost, convenience and accuracy. Whilst on the topic of non-response, it is worth mentioning that the variable with greatest amount of non-response in the Wave 50 data set is household income at roughly 5.5%. Nonetheless, this is a much smaller percentage of non-response in household income than is typically seen in political surveys, and Nationscape handles it by not weighting income for non-respondents. Similarly, a statement from Nationscape in its "Representativeness Assessment" implies an overall lack of concern for non-response in the data, as "previous evaluations of the samples Lucid provides have found them to be of high quality (Tausanovitch, 2020)." 
  
  The last aspect of the Nationscape Wave 50 dataset to be considered is the survey questionnaire itself. Strengths of the questionnaire include its structure, the design and wording of the questions being asked, and the variety of choices for closed-ended questions. As to the structure of the questionnaire, it contains an extensive amount of questions from which differing political views are revealed -- the ideal circumstance for reflecting on American voter attitudes. Furthermore, the survey questions are asked in an order that does not influence the answer of respondents to subsequent questions. In terms of the design and wording of the questions, they are closed-ended, and avoid the use of tricky wording. This is a strength of the questionnaire because it makes comparing the responses of individuals and thus, statistical analysis, much simpler than it would be with open-ended questions and/or troublesome language. The diverse number of options available to select as answers to the questions is a strength of the questionnaire because they are both mutually exclusive and exhaustive; hence, ensuring survey respondents are able to answer every question. With regard to weaknesses of the questionnaire, it seems that there are none other than its sheer length.
  
 The data was cleaned to get our target population of eligible voters that intend to vote in the 2020 elections. Voters who didn't plan to vote or not eligible were filtered from the data set, and the variable pertaining to the 2020 election vote was cleaned to consist only of voters who plan on voting for either Trump or Biden. Before the process of fitting our model, we selected variables of interest that we felt gave insight on how different demographics vote. The resulting data set consisted of 3541 observations, and later reduced to the variables state, age, sex, Hispanic, race, and intended 2020 vote.

```{r ageind, echo=FALSE, fig.cap="Distribution of Age in Nationscape data", message = FALSE}
individual_survey <- readRDS(here::here("inputs/cleaned_data","individual-survey.rds"))
post_strat <- readRDS(here::here("inputs/cleaned_data","post-strat.rds"))

cont <- mutate(individual_survey, vote = (case_when(individual_survey$vote_2020 == 1~"Trump", individual_survey$vote_2020 == 0~"Biden")))

  ggplot(cont, aes(x = age, fill  = vote), color = "black") + 
  geom_bar(alpha = 0.7, color = "black") + 
  scale_fill_manual(values = c("#0000FF","#FF4040")) + 
  labs(title = "Age Vote Distribution" , 
       subtitle =  " A look into how eligible voters intend to vote based on age.", 
       x = "Age", 
       y= "Number of votes", 
       fill = "Nominee") +   
  theme(axis.text.x = element_text(angle = 35, hjust = 1, size = 5))

s <-cont %>% 
  select(vote_2016, race) %>% 
  group_by(race, vote_2016) %>% 
  summarise(n = n()) %>% 
  mutate(freq = n/sum(n))
```

Age:  The age of a respondent. Age was originally a nominal value(numerical), but the data was later grouped into categories for our predictive model, and to match the post-stratification data. This allowed for us to look at how age groups might influence the vote. The 2020 election marks the first time that Millennials and Gen Z will have an equal share of the number of eligible voters as Baby Boomers and earlier generations [@agearticle]. Older voters in the past have had higher turn out rates, so looking at how how age categories tend to vote can give insight on how this will play a factor in the future election. We can see in (Figure \@ref(fig:ageind)) that there is a much smaller proportion of younger voters, but this is the age group that seems to have the strongest Democratic support. 


Sex: The sex of a respondent. Certain policies currently being such as those on anti-Abortion largely impact the female population in the United States. It seems that sex can be an important indicator of how voters respond to the platforms of the parties with regards to sex-specific policies. 

Hispanic: This variable looks at whether a respondent is Hispanic or not. In the 2020 election, Latinos will be the largest racial/ethnic minority in the electoral. With Trump's previous comments on countries such as Mexico, it seems important to see how pivotal the vote of this group will be in the election [@citeBBC].

```{r racedis, echo=FALSE, fig.cap= "Votes by Race in Nationscape data"}
#these graph were done as EDA to see if there were any notable trends

## 2016 vote by race
s %>% ggplot(aes(x = vote_2016, fill = race, group = race)) + 
  geom_bar(aes(y = freq), stat = "identity", position = "dodge", alpha = 2/3) + 
  labs(x = "Race", title = " Percentage of 2016 Presidential Election Votes by Race")
```

Race: (Figure \@ref(fig:racedis)) shows the distribution of votes in the 2016 federal election; specifically the Democratic candidate, Hillary Clinton, and the Republican candidate, Donald Trump. We can see that certain groups had a significant different in their voting preference. for example, majority of Black voters voted for Clinton rather than Trump, as well as the population of Chinese voters.

State: When forecasting the election outcome, we will be predicting by State to see the majority vote in each. Certain states have been dominated by one particular party. For example, the Democratic party hasn't won the majority vote in the presidential election since 1992 [@cite270towin]. Since the election is based on electoral colleges as well, the number of Sates won by a candidate should give us a clear prospect for the dominating candidate. 


## Post-stratification Data

  For our next step in our analysis, we use post-stratification to overcome any bias and have data representative of our target population. Conducted on a yearly basis, the American Community Survey "is a nationwide survey that collects and produces information on social, economic, housing, and demographic characteristics" of the United States population. In order to do so, the United States Census Bureau "mails questionnaires to approximately 295,000 addresses a month across the [country]." It should be noted that all the addresses receiving a letter to participate in the ACS are from a random sample chosen by the Census Bureau each month. Furthermore, every single address in the United States has approximately a 1 in 480 chance of being randomly selected to participate in the ACS during a given month. The exception to this comes from addresses that have already been selected to participate in the last 5 years; they are not allowed to be selected for another monthly sample. 
  To access the 2018 1-year ACS dataset, the first step is to visit https://usa.ipums.org/usa/ and create an account. From there, IPUMS will send you an email to verify your email address. By clicking the link contained in the email, you will be redirected to the IPUMS website and your email will be verified. Once on the website, you will be able to access the page where ACS sample and variables can be selected. To choose the 2018 1-year ACS as your sample, press "Select Samples". For this analysis, the single 2018 sample was chosen. The website allows for the user to choose the sample, as well as any variables of interest. After selecting the sample, you will be taken to a page wherein variable categories are listed. In addition, there is also an option to search for variables of interest. The variables that we selected for the dataset used in this paper are: region, stateicp, hhincome, sex, age, marst, race, hispan, bpl, citizen, educ, labforce, and ftotinc. When finished selecting variables, click "View Cart" near the top right of the page, and you will arrive at the cart page. This is your chance to look over all of the chosen variables. If you are happy with them, press the blue button near the middle of the page that reads "Create Data Extract." On the new page that you arrive at, there is a blue button near the bottom of the page with "Customize Sample Sizes" displayed in it. If you wish to change the size of your sample, then click the button. Now that our dataset has been fully customized, press the blue "Submit Extract" button at the very bottom of the page. Finally, you have reached the page from which your dataset can be downloaded, and the final step is to wait for the sample to be ready. 
  
  
  Regarding the survey methodology employed by the Census Bureau for the ACS, they do not find people to survey, but instead find households. The target population of the ACS is the population of the United States, and the sampling frame consists of all Americans living at a specific address. The sample itself includes all Americans living at one of the 295,000 addresses that are selected to participate in the ACS during a given month. Furthermore, the Census Bureau designs the sample in a way that guarantees favourable geographic inclusion, while also utilizing random sampling to select the "addresses to be included in the ACS." Every resident of the addresses which the invitation to complete the ACS was mailed is required to complete either the online survey, or "to mail the completed paper questionnaire." If neither requirement is satisfied after about a month, the Census Bureau "will mail an additional paper survey questionnaire." If the second paper questionnaire goes uncompleted too, then roughly 1 month after it was sent, someone will show up at your address to conduct an in-person interview with you. Due to the fact that completing the ACS is the law in the United States, non-response does not appear to be a very big issue. In terms of the ACS sampling approach, there are definitely some trade-offs made. For instance, although random sampling can cause sampling bias in the data, this concern is outweighed by the benefits of the sampling approach used. A few obvious benefits of the sampling approach that come to mind are a lower cost of administering the ACS, as well as a widespread geographic representation in the data. 
  
  In addition to the strength of the ACS dataset with respect to its customizability, another is the timeliness of the data. To be precise, survey data was continuously collected throughout 2018, and the results were shared in 2019. This is very important as it means there is opportunity to gain a more thorough understanding of current characteristics in the United States than has ever before been possible. One of the only obstacles in achieving this comes from the main weakness of the ACS dataset--poor data quality. For example, there are numerous duplicate records and inappropriate entries contained in the data. Some potential explanations for the low standard of data are measurement error, processing error, and/or adjustment error. More explicitly, measurement error is a possible reason for the bad data because when American citizens are selected to complete the ACS survey, they are required to complete it by law. Although the survey can be completed online, or the completed questionnaire can be mailed back, for someone who never wanted to complete the survey to begin with, it would not be a big surprise for them to quickly record poor quality answers and submit it. Processing error could be contained in the data as a result of either an error in coding with respect to the online survey, or an error in data entry by the person inputting completed paper questionnaires to the computer. Lastly, adjustment error serves as a possible explanation for the low quality data since any adjustments made to past survey results could have had an adverse affect.
  

# Model

In order to forecast the popular vote of the U.S 2020 elections, multilevel logistic regression with post-stratification (MRP) was used. MRP is useful for when generalizing from a possibly non-representative poll {kennedy2020know}.  The individual survey gives sample data on voters of the general U.S population, and post-stratification allows us to re-weight estimates, adjusting bias between our sample and the target population, so that we have a representative sample of likely voters {alexander2020a}. To achieve this, cells are constructed using variables in the cleaned Nationscape survey such as age, household income, and race. The model is then trained on the survey using the proportions given by the  ACS post-stratification data set. This approach gives us an advantage when attempted to forecast voting, as we can use a broad survey to speak to subsets in the population, and also tends to be less expensive to collect than non-probability samples {alexander2020a}. However, using this approach places limitations on how much we can interpret from the estimates. Although we can estimate voting intention in different demographic groups, it doesn't tell us any qualitative information on voting patterns. For example, we might be able to see how different age categories vote, but we won't have a measure of the policy preferences, or their views on the candidates.

Using the Nationscape data, a multilevel logistic regression model can be fitted to the survey data set to model the proportion of voters who will vote for Trump. Logistic regression can be used to model a binary dependent variable, which is the choice between the Republic candidate, Donald Trump, and the Democratic candidate, Joe Biden in our case. This would be the main reason for choosing this model, over another model such as a linear regression model, where the output isn't necessarily binary. However, this imposes limitations. As we cannot take into consideration any other candidates running in the election due to the binary outcome. The variable 'vote_2020' in the data set was used as the dependent variable (the variable we are trying to predict), with the variable returning 1 for voters intending to vote for Trump, and 0 if they are voting for Biden. 


Our model uses the variables pertaining to age, state, race, sex, household income, and whether a voter is Hispanic to predict whether a respondent will vote for Trump or Biden in the elections. 
The model takes the form:
$$ Pr(y_i = 1) = logit^-1(X_i *\beta) $$
Equation from: {@RAOS}

where $y_i = 1$ represents a voter voting for Trump in the election.
$Pr(y_i = 1)$ is the probability of a voter choosing to vote for Trump.
The $X_i\beta$ are the linear predictors, where $\beta$ represents the fitted coefficients for each independent variable $X_i$ in the model. The coefficient values signify the mean variable changes in a one unit shift in the given variable $\beta$.


The logit function $logit(x) = log(x/1-x)$ maps the range (0, 1) to $(-\infty, \infty)$.
Its inverse function $logit^-1(x) = e^x/(1+e^x)$ maps back to the unit range. 
The model's output is bounded between 0 and 1,  mapping the outputs into a binary outcome. The output tells us the probability that some person $i$, will vote for Trump depends on their age, sex, state, household income, and whether the voter is Hispanic. 

We fit our regression model using the `glm()` function in [`R`].  We can see from table 1, which was made using `kable` from `knitr`,  that the fitted model results in the following coefficients:
```{r, echo=FALSE}
# loading data sets and cell counts for MRP
cell_counts <- readRDS(here::here("inputs/cleaned_data", "cell_counts.rds"))


#Loading in the final model and the coefficents of the model in this cell. Please note that the outputs of those models are being stored in the outputs/model directory. The here package should give you the same result but if it doesn't then you may have to manually enter the same directory.



final_model <- readRDS(here::here("outputs/model","final_model.rds"))
coefs <- readRDS(here::here("outputs/model","coefficients.rds"))
knitr::kable(coefs, caption = " Table 1: Model Coefficients", digits = 2) 

```
In logistic regression, collinearity implies that we have predictor variables that are highly correlated; that is, they have a linear relationship. This is possible when you have a larger number of variables in a model, but having collinearity in multiple variables can lead to unstable estimates and inaccurate variances, which can affect confidence intervals. This could lead to incorrect inferences about the relationship between our response variable and explanatory variables. Thus, it is imperative to check for any correlated variables in our model to avoid these issues. The value of the Variance Inflation Factor(VIF) in for the explanatory variables can give us a measure of collinearity our model. VIF values should be less than 5 to guarantee that collinearity is not an issue. We can see in table 2 below that all VIF values are less than 5, so collinearity will not be a concern for our model. 
```{r, echo=FALSE}
cor <- readRDS(here::here("outputs/model","cor.rds"))
knitr::kable(cor, caption =  "Checking Model Collinearity", digits = 2)
```

```{r, echo=FALSE}
 htst <- hoslem.test(final_model$y, fitted(final_model), g=10)
```
Before the model can be used for any predictions, we must also check that our model fits our data well, and meets any assumptions made by the model. The Hosmer-Lemeshow goodness of fit test assesses whether or not the observed events match expected events in subgroups of the model. For this test, the p-value can be used to assess goodness of fit. A smaller p-value indicates strong evidence against a good fit. In the case of our model, we can see in table 4 that we have a very large p-value of 0.9, indicating no evidence of poor fit.

| Table 4: Hosmer and Lemeshow goodness of fit (GOF) test 	|    	|         	|
|---------------------------------------------------------	|----	|---------	|
| X-squared                                               	| df 	| p-value 	|
| 3.5                                                     	| 8  	| 0.9     	|

```{r, echo=FALSE, include = FALSE}
# make predictions using our model with data from post-strat data set

cell_counts


cell_counts$estimate <- predict(final_model, newdata = cell_counts, type = "response")
#  prediction intervals
# fitted value - error to get margin of error
se_md <- predict(final_model, newdata = cell_counts, type = "response", se.fit = T)

 lwr <-  cell_counts$estimate - 1.96*se_md$se.fit
 upper <- cell_counts$estimate + 1.96*se_md$se.fit

# all estimates with lower and upper bounds
estimates_with_margins <- cell_counts %>% mutate(n = num_records * estimate) %>% cbind(cell_counts, lwr, upper)
# removing duplicate column name estimate
estimates_with_margins <- estimates_with_margins[, !duplicated(colnames(estimates_with_margins))]

# post-stratified estimates - number of trump votes with prediction intervals
post_strat_estimates <- estimates_with_margins %>% 
  mutate(predict_prop = prop * estimate, l_predict = lwr * prop, u_predict = upper * prop) %>% 
  group_by(state_name) %>% 
  summarise(predicted_vote = sum(predict_prop), lower = sum(l_predict), upp = sum(u_predict)) %>% group_by(state_name) %>% ungroup()




post_strat_estimates <- post_strat_estimates %>% 
  mutate(biden_vote_proportion = 1-predicted_vote,
         conservative_election_status = if_else(lower %>% between(0.48,0.52),"toss-up",
                                                 if_else(lower > 0.52, "win","loss")),
         liberal_election_status  = if_else(upp %>% between(0.48,0.52),"toss-up",
                                                 if_else(upp > 0.52, "win","loss")),
         middle_ground_election_status = if_else(predicted_vote %>% between(0.48,0.52),"toss-up",
                                                 if_else(predicted_vote > 0.52, "win","loss")))

```

# Results

All our analysis was done using R {@CiteR}.


The graphs for our variables are done in similar fashion for each of our variables. First, the difference in their distributions between both datasets is displayed. Then, showing the relationship between those variables and voting preferences is shown. The purpose of such a display is to showcase the effect of the variables, and then how a change in the distribution will affect the result. For instance, we may observe that women are more likely to vote for Biden, and then subsequently we notice that there are more women in the ACS census. We will go in-depth about the implications of these observations in the discussion section.


## Comparison on Age
```{r age, echo=FALSE, fig.cap="Distribution of Age between ACS and Nationscape data", message = FALSE}
individual_age_dist<- individual_survey %>% 
  group_by(age) %>% 
  summarise(num_records = n()) %>% 
  mutate(percentage = 100 * num_records/sum(num_records),
         total_num = sum(num_records),
         data_source = "Nationscape")

post_strat_age_dist <- post_strat %>% 
  group_by(age) %>% 
  summarise(num_records = n()) %>% 
  mutate(percentage = 100 * num_records/sum(num_records),
         total_num = sum(num_records),
         data_source = "ACS")

individual_age_dist %>% 
  union(post_strat_age_dist) %>% 
  ggplot(aes(x = age, y = percentage)) + 
  geom_col() + 
  facet_grid(~data_source) + 
  theme_economist_white()
```
Looking at (Figure \@ref(fig:age)), we observe a shift in age distributions between the two datasets. There is a slight peak in the 25-44 age group in the nationscape data. We see this flatten and then observe a significant increase in the 65+ age group, and a minor increase in the 18-24 age groups.



## Comparison on Sex

```{r, echo = FALSE,  message = FALSE}
individual_sex_dist<- individual_survey %>% 
  group_by(sex) %>% 
  summarise(num_records = n()) %>% 
  mutate(percentage = 100 * num_records/sum(num_records),
         total_num = sum(num_records),
         data_source = "Nationscape")

post_strat_sex_dist <- post_strat %>% 
  group_by(sex) %>% 
  summarise(num_records = n()) %>% 
  mutate(percentage = 100 * num_records/sum(num_records),
         total_num = sum(num_records),
         data_source = "ACS")

individual_sex_dist %>% 
  union(post_strat_sex_dist) %>% 
  ggplot(aes(x = sex, y = percentage)) + 
  geom_col() + 
  facet_grid(~data_source) + 
  theme_economist_white()

```


```{r sex_vote, fig.cap="Which individual did the candidate vote for? 2020 on the left, 2016 on the right", echo = FALSE, message = FALSE}
#figure to see 2020 voting by sex
vote_2020_sex <- individual_survey %>% 
  mutate(vote_2020 = if_else(vote_2020 == 1,"Donald Trump","Joe Biden")) %>% 
  group_by(sex,vote_2020) %>% 
  tally() %>% 
  ggplot(aes(x=sex, y = n)) + 
  geom_col() + 
  facet_grid(~vote_2020)+ labs(x = "Sex", y = "Number of people")


#figure to see 2016 voting by sex
vote_2016_sex <- individual_survey %>% 
  group_by(sex,vote_2016) %>% 
  tally() %>% 
  ggplot(aes(x=sex, y = n)) + 
  geom_col() + 
  facet_grid(~vote_2016) + labs(x = "Sex", y = "Number of people")



library(patchwork)
vote_2020_sex + vote_2016_sex
```

(Figure \@ref(fig:sex_vote) indicates that a man is much more likely to vote for Trump than a woman is. This trend is also observed in the 2016 elections when Hillary Clinton, a woman, ran for office.





```{r survey_election_results, echo = FALSE}
individual_survey %>% mutate(vote_2020 = if_else(vote_2020 == 1,"Donald Trump","Joe Biden")) %>% group_by(vote_2020) %>% tally() %>% rename(candidate = vote_2020, votes = n ) %>% knitr::kable(caption = "Polling results for 2020 Election")
```
The above table indicates that Trump is most likely to win. This is captured from Nationscape data.


```{r model2020prediction}
post_strat_estimates %>% group_by(middle_ground_election_status) %>% tally() %>% rename(state_result = middle_ground_election_status, number = n) %>%  knitr::kable(caption = "Model Prediction for 2020")
```



```{r, echo = FALSE, message = FALSE} 

# estimated proportion of votes by State
post_strat_estimates %>%
ggplot(aes(y = predicted_vote, x = state_name, color = "MRP estimate")) + geom_point() +
geom_errorbar(aes(ymin = lower, ymax = upp), width = 0) +
ylab("Proportion of Trump Supporters") +
xlab("State") +
geom_point(data = individual_survey %>%
group_by(state_name, vote_2020) %>% summarise(n = n()) %>% group_by(state_name) %>%
mutate(prop = n/sum(n)) %>% filter(vote_2020==1),
aes(state_name, prop, color = "Raw data")) + theme_minimal() +
scale_color_brewer(palette = "Set1") + theme(legend.position = "bottom") + theme(legend.title = element_blank()) + 
  theme(axis.text.x = element_text(angle = 35, hjust = 1, size = 5)) + coord_flip() +labs(title = "2020 Predicted Popular Vote by State", subtitle = "Comparing predicted popular vote by state using raw data and MRP")
```

```{r heatmap, echo = FALSE, fig.cap="Heatmap showcasing model predictions at a state-level"}
# Sorting State wins
election_wins <- 
  post_strat_estimates %>% 
  mutate(winner = if_else(predicted_vote>=0.50,"Trump","Biden")) %>% 
  group_by(state_name)

election_wins$state <- str_to_title(as.character(election_wins$state_name))

election_wins <- election_wins %>% filter(state != "district of columbia")

# Plotting state wins
statebins(election_wins, value_col="winner", ggplot2_scale_function = scale_fill_manual, values = c(Trump ="#b2182b" , Biden = "#2166ac"), name = "state") +
  theme_statebins(legend_position="right")



```

Figure(\@ref(fig:heatmap)) shows the predictions of our model on the post-stratified dataset. This figure ignores close calls and has a binary Trump or Biden result.



# Discussion

While American citizens play a significant role in electing the next President, it is safe to say that the results of the US federal election are watched closely by the entire world. It goes without saying that the results of this election will have implications around the world. To contextualize the results of our model and analysis, we need a quick and simplified lesson on the American electoral process. Unlike most democracies, such as Canada, the head of state is typically elected by a popular vote. In the US, however, the head of state is not elected by a popular vote. Instead, citizens vote for a representative, or an elector, that then elects the president. This group of electors forms the electoral college. The electoral college is tasked with electing the next President. If a candidate wins a state by even a small percent, they effectively win all the electors in the state. 

with that background knowledge in mind, what do our poll results indicate? From Figure(\@ref(fig:survey_election_results)), we gather that Trump has a very slight lead. Multiple other polls have shown Biden to be in the lead(FiveThirtyEight, 2020). Now there are a few reasons as to why we may not want to use poll results to predict elections. Polls have of late garnered a lot of criticism due to their incorrect predictions of the 2016 Federal election results[@citeAapor]. Note that while they got the ultimate results wrong, they did get the popular vote prediction correct[@citeAapor].Nearly every national poll that year had Hillary Clinton winning, with some estimates of her having a 3% point lead[@citeAapor]. Ultimately, she won the popular vote by 2.1%. However, as mentioned previously, the US does not use a popular vote system. At the state level, the polls became less effective, indicating a Clinton lead again[@citeAapor]. State level polls do translate over to the electoral college a bit more effectively than national polls do. Another reason we may not want to overly rely on polls is yet another cautious tale from the 2016 election season. In that season, polls had over-represented college-graduates, a group that was more likely to vote for Clinton, thus tipping the scales even more in favour of Clinton[@citeAapor]. Capturing this over-representation is key to effectively predicting an election. As mentioned in the results section, some results were intentionally shown as their distributions and then their voting preferences. This was done to elucidate the effect of a difference in representation, akin to  the difference of representation of educated voters between polled data and actual populations. 
From figure(\@ref(fig:age)), there is a clear shift in the distribution of ages. The increased representation of ages 65+ would likely predict a Trump win. Our model coefficient results also indicate that someone being in the oldest age group is more likely to vote for Trump, though significance is only slightly off. With an effectively older population compared to the Nationscape polling data, we can conclude that we would be likely see more votes for Trump coming out of our ACS group. Similarly, when looking at sex, we observe that there are more women in our ACS dataset than there are men, which is the opposite of the distribution found in the Nationscape data. Our model coefficient results again suggest that men are more likely to vote for Trump, and since we have more women in the ACS data, we would expect a higher percentage of Biden votes. This is further supported by other research that suggests that Trump has lost favour with white suburban women, a group he previously was well percevied by[@conroy2020]. A lot of models during the 2016 election season suggested a Clinton win, one of the few models that predicted a Trump win was a model that used MRP[@2016election]. It is worth mentioning that MRP isn't some sort of magic bullet as a few other MRP models during that time still predicted a Clinton win within the electoral college[@2016election]. How do current models fair? The primary model suggests that Trump has a 91% change of winning, in spite of Trump trailing Biden in the polls[@norpoth_2020]. 

Overall, our model predicts a Trump majority. The

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

# Appendix {-}

\newpage


# References

Tausanovitch, Chris and Lynn Vavreck. 2020. Democracy Fund + UCLA Nationscape, October 10-17, 2019 (version 20200814). Retrieved from https://www.voterstudygroup.org/publication/nationscape-data-set.



Steven Ruggles, Sarah Flood, Ronald Goeken, Josiah Grover, Erin Meyer, Jose Pacas and Matthew Sobek. IPUMS USA: Version 10.0 [dataset]. Minneapolis, MN: IPUMS, 2020. https://doi.org/10.18128/D010.V10.0